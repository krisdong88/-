{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence to Sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最早是用来做翻译，就是给一个源语言的句子，自动翻译成目标语言，以前用Seq to Seq现在可能用Bert，是一个encoder和decoder的架构，encoder就是给一个句子，比如英国的hello world然后要翻译成法语的句子，原文里面，编码器用的是RNN就是seq进去，双向RNN一般都用在encoder里面，解码器就是也是个RNN会获得原句子的隐藏状态然后一直重复的输出，知道看到eos停止。不管原句子或者目标句子的长度，因为RNN encoder会一直获取知道最后eos获得句子的隐藏状态停止，像decoder就会一直一直生成，知道eos结束。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是原句子到一个任意长度的句子的翻译所以叫sequence to sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体是把最后一层的rnn在最后那个时刻的隐藏状态，和句子embedding的输入弄起来，作为decoder的输入，就说白了就是把encoder的最后一层放到decoder的第一层，作为decoder的初始状态和context，所以encoder甚至不需要全连接层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder在训练的时候在每一个rnn输入的时候都是正确的输入，这样就不会出现预测长了就不行了，所以是真正的句子用来训练。推理的话就是没有真正的句子作为上一个时刻的输入，所以就只能靠上一层的输出来作为下一层的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器翻译里面最重要的值就是BLEU。 pn就是用来预测n-gram的精度 我考虑我预测李安的所有uni-gram然后看是不是在标签序列里面出现过。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算方式我记住了，晚点写，假设预测的长度比真实的长度小很多的话，那么这个数就大于1，0.1-这个数就是负数了，然后exp以后就非常小，BLUE肯定越高越好。 预测比较长n越大权重就越大用来处理两个句子的匹配，并且两个句子肯定不一样长。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq 是从一个句子生成另一个句子\n",
    "编码器和解码器都是RNN\n",
    "句子翻译机器翻译一般都会用BLEU来分辨句子的好坏。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
