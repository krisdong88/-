{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Normalization和Batch Normalization的区别："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一句话版本：\n",
    "\n",
    "Batch Normalization 依赖于整个批次的数据来进行归一化，适合CNN 如（AlexNet https://zhuanlan.zhihu.com/p/133712585，VGG https://zhuanlan.zhihu.com/p/42233779，ResNet https://zhuanlan.zhihu.com/p/56961832），而 Layer Normalization 依赖于单个数据点的所有特征，适合RNN（https://zhuanlan.zhihu.com/p/30844905）和Transformer(https://zhuanlan.zhihu.com/p/338817680)模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "详细版本：\n",
    "\n",
    "Batch Normalization（批量归一化）和 Layer Normalization（层归一化）都是深度学习中常用的归一化技术，用于加速模型训练和改善模型的泛化能力。虽然这两种技术的目标相同，即通过调整神经网络中间层的输出来稳定学习过程，但它们在归一化的维度和应用场景上有所不同。\n",
    "\n",
    "Batch Normalization（批量归一化）\n",
    "Batch Normalization（BN）是在每个训练批次的基础上对每个特征单独进行归一化。具体而言，BN 对每个特征（在卷积网络中是每个通道）计算批次中所有样本的均值和标准差，并用这些统计数据来归一化相应的特征。\n",
    "\n",
    "优点：\n",
    "\n",
    "加速收敛：通过减少内部协变量偏移（Internal Covariate Shift），BN 可以让模型使用更高的学习率，从而加速收敛。\n",
    "提供一定的正则化效果：由于批量归一化的随机性（每个批次的样本不同），它能提供轻微的正则化效果。\n",
    "缺点：\n",
    "\n",
    "依赖批次大小：BN 的效果依赖于较大的批次大小，因为每个批次内需要足够的样本来计算可靠的均值和标准差。\n",
    "性能下降问题：在小批量数据上可能会导致估计的均值和标准差不准确，影响模型表现。\n",
    "Layer Normalization（层归一化）\n",
    "Layer Normalization 则是对单个样本的所有特征进行归一化，通常应用于递归神经网络（RNN）和 Transformer 类模型。与 BN 不同，LN 不依赖于批次的大小，它在单个样本的所有激活上计算均值和标准差。\n",
    "\n",
    "优点：\n",
    "\n",
    "不依赖批次大小：LN 的计算只依赖于单个样本，因此适用于批次大小较小或者动态变化的批次大小的情况。\n",
    "适用于RNN和Transformer：由于其计算方式，LN 在处理时间序列数据和处理长距离依赖的模型中表现更好。\n",
    "缺点：\n",
    "\n",
    "可能不如BN有效：在某些情况下，特别是在卷积神经网络（CNN）中，LN 可能不如 BN 有效，因为它不利用批次中样本之间的统计信息。\n",
    "应用选择\n",
    "Batch Normalization 更适用于卷积神经网络和较大批次的场景。\n",
    "Layer Normalization 更适用于处理时间序列数据的模型，如 RNN 和 Transformer，以及批次大小不一或非常小的场景。\n",
    "归一化技术的选择取决于具体的应用场景和模型架构。理解它们的工作原理和适用性可以帮助你更好地设计和优化深度学习模型。如果你有更多关于这些技术的问题或者需要进一步的解释，请随时提出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN要怎么计算维度？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一句话版本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "假设你有一个输入层的尺寸为 32(H)×32(H)，使用5（F)×5(F) 的滤波器，S(步长)为 1，没有填充（P=0）。 输出层的维度就是28，因为 Floor((H + 2P - F)/S + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络模型不收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原因\n",
    "1.忘记对你的数据进行归一化\n",
    "\n",
    "2.忘记检查输出结果\n",
    "\n",
    "3.没有对数据进行预处理\n",
    "\n",
    "4.没有使用任何的正则化方法\n",
    "\n",
    "5.使用了一个太大的 batch size\n",
    "\n",
    "6.使用一个错误的学习率\n",
    "\n",
    "7.在最后一层使用错误的激活函数\n",
    "\n",
    "8.网络包含坏的梯度\n",
    "\n",
    "9.网络权重没有正确的初始化\n",
    "\n",
    "10.使用了一个太深的神经网络\n",
    "\n",
    "11.隐藏层神经元数量设置不正确\n",
    "\n",
    "对应的解决办法分别是：\n",
    "\n",
    "1。对数据进行归一化，常用的归一化包括零均值归一化和线性函数归一化方法；\n",
    "2。检测训练过程中每个阶段的数据结果，如果是图像数据可以考虑使用可视化的方法；\n",
    "3。对数据进行预处理，包括做一些简单的转换；\n",
    "4。采用正则化方法，比如 L2 正则，或者 dropout；\n",
    "5。在训练的时候，找到一个可以容忍的最小的 batch 大小。可以让 GPU 并行使用最优的 batch 大小并不一定可以得到最好的准确率，因为更大的 batch 可能需要训练更多时间才能达到相同的准确率。所以大胆的从一个很小的 batch 大小开始训练，比如 16，8，甚至是 1。\n",
    "7。不采用梯度裁剪。找出在训练过程中不会导致误差爆炸的最大学习率。将学习率设置为比这个低一个数量级，这可能是非常接近最佳学习率。\n",
    "8。如果是在做回归任务，大部分情况下是不需要在最后一层使用任何激活函数；如果是分类任务，一般最后一层是用 sigmoid 激活函数；\n",
    "9。如果你发现你的训练误差没有随着迭代次数的增加而变化，那么很可能就是出现了因为是 ReLU 激活函数导致的神经元死亡的情况。可以尝试使用如 leaky ReLU 或者 ELUs 等激活函数，看看是否还出现这种情况。\n",
    "10。目前比较常用而且在任何情况下效果都不错的初始化方式包括了“he”，“xaiver”和“lecun”。所以可以任意选择其中一种，但是可以先进行实验来找到最适合你的任务的权值初始化方式。\n",
    "11。目前比较常用而且在任何情况下效果都不错的初始化方式包括了“he”，“xaiver”和“lecun”。所以可以任意选择其中一种，但是可以先进行实验来找到最适合你的任务的权值初始化方式。\n",
    "12.从256到1024个隐藏神经元数量开始。然后，看看其他研究人员在相似应用上使用的数字\n",
    "\n",
    "Quote from https://github.com/zonechen1994/CV_Interview/blob/main/%E9%80%9A%E7%94%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E9%9D%A2%E7%BB%8F/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%98/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8D%E6%94%B6%E6%95%9B.md"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
