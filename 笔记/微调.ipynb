{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总的来说微调一共分成四个步骤\n",
    "1.在原来的数据集比如ImageNet预训练一个神经网络作为backbone，因为这样可以让原来的模型学习到很多特征一般都会使用现有的与训练大模型，在CV里面就会使用比如Vit-Large,ResNet-152之类已经在ImageNet数据集上训练并且测试良好的模型来作为源模型。这样公司可以在前面节约很多时间并且可以省下很多的计算资源。就打个比方，这些大公司用了很多钱和资源训练出来的模型就是车厂刚造出来的车子，而微调的作用就是在有限的资源上面来对这辆汽车进行改装这样可以适配更多的路况，比如兰博基尼越野车。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.创建一个新的神经网络模型，即目标模型，它复制了源模型上除了输出层一味的所有模型设计以及参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.为目标模型添加一个输出大小为目标数据集类别个数的输出层，并且随机初始化该层的模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.在目标数据集上训练目标模型，从头开始训练输出层，而其余层的参数都是基于源模型的参数微调得到的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当目标数据集小于源数据集的时候，微调会有助于提升模型的泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在的大模型有个问题就是大公司可以开源已经训练好的大模型但是对于个人和小型企业来说是不可能做到的，因为每次的数据训练量和价格都是非常高的就比如GPT这种模型训练一次就要很多很多钱，所以小型公司可以使用大模型然后在这个基础上进行微调这样的话小型企业或者个人就可以使用这些大模型来应用到自己的垂直领域来了，然后现在主流的方法就包括2019年 Houlsby N 等人提出的 Adapter Tuning，2021年微软提出的 LORA，斯坦福提出的 Prefix-Tuning，谷歌提出的 Prompt Tuning，2022年清华提出的 P-tuning v2这些方法都有各自的特点，从个人使用情况来说，LORA 的效果会好于其它几种方法。其它方法都有各自的一些问题：Adapter Tuning 增加了模型层数，引入了额外的推理延迟 Prefix-Tuning 难于训练，且预留给 Prompt 的序列挤占了下游任务的输入序列空间，影响模型性能\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lora 微调 是一种轻量级的微调方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
